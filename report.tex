% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%% Use the graphicx package to include figures.
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Examining Large Language Model Calibration for Question Answering}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Aakarsh Nair\\
  University of Tuebingen / Geschwister-Scholl-Platz, 72074 TÃ¼bingen\\
  \texttt{aakarsh.nair@aakarsh.nair@student.uni-tuebingen.de} 
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\begin{document}
\maketitle

\begin{abstract}
In this paper, we examine the issue of calibration of large language models. 
That is the interaction between the \emph{confidence} of a predicted answer 
on a question-answering task with its \emph{empirical likelihood 
of being correct}.

We replicate elements of previous calibration study \cite{kadavath2022language} 
on several multiple-choice  (MMLU, LogicQA, TruthfulQA) and 
open-ended question answering datasets translated into 
the multiple choice format (TriviaQA, HumanEval, GSM8k). 

We find that models do scale in their calibration ability by model size. 
Moreover models fine-tuned for conversation improve in calibration and 
accuracy under multi-shot prompting. However, we also observe that for tasks beyond 
models reasoning  capability (complex logical and scientific reasoning) fine-tuning harms models 
accuracy and leads to overconfidence in model predictions.

\end{abstract}


\section{Introduction}

Understanding the reliability and correctness of language model (LM) generations is crucial in ensuring their practical utility and trustworthiness. One significant aspect of this inquiry pertains to the calibration of LMs and their ability to accurately indicate uncertainty about their outputs. This issue has gathered considerable attention in recent research, particularly within the context of large-scale language models.

In the paper, Kadavath et al. [2022], it extensively examined the calibration of big base LMs, demonstrating their well-calibrated nature. Their study revealed that the probabilities assigned to answer options on benchmark datasets such as BIGBench and MMLU correlated effectively with the correctness probabilities across trials. However, investigations into reinforcement learning-based language models (RL-LMs) have raised concerns about calibration deterioration following fine-tuning [OpenAI, 2023; Kadavath et al., 2022], albeit with inconsistent findings.

Moreover, an additional challenge in LM-generated content relates to the occurrence of hallucinations, where the model generates false or improbable information. To address this, fine-tuning LMs with reinforcement learning has been suggested, with the goal of prompting responses like "I don't know" to receive high rewards. However, concerns have been raised regarding the potential evasiveness of such models. The present project seeks to provide more comprehensive understanding and explore various aspects of calibration in RL-LMs.





%% Introduction, Motivation and Explanation of the 
%% research question in the context 
%% of the field / the class, explaining why it is 
%% intersting / novel in comparison 
%% to extant related work. Note that it is NOT mandatory to include an 
%% exhaustive literature review; it is sufficient to mention relevant work 
%% from the project proposals and the class materials

\section{Methodology}

\subsection{Introduction}
%% Not enough mathematical explanation of what the term mean. 
%% Not explaining, what log completion probbilty means. 
%% No explantation of transfoerm completion. 

We measure the calibration in keeping with the methodology presented 
in  \cite{kadavath2022language}. The model is queried in either a
0-shot or 5-shot manner. The transformer model is given a 
question prompt  with the each of the multiple choice options.  

In the 5-shot methodology, in addition to the question under 
test we proceed the question with 4 additional questions 
in the same multiple choice format in order to provide 
the model with enough context to understand the expected format for answering the questions.

For each prompt completion pair we compute the log probability 
of  the completion normalized by its completion length. 

The computed probabilities are then grouped in bins in 
equally weighted bins from  $0$ to $1$ were we compute 
the average frequency of answering correctly for each bin. 

For ideal calibration, that is when the 
model completion probability aligns closely with the actual 
likelihood of answering the question correctly, these 
two computed probabilities must be equal, thus an 
ideal calibration would be represented  by a line of 
slope $1$ in the calibration plot. 
Negative and positive deviations thus represent a model 
which is under-confident  or over-confident  in its answers respectively.

We judge the models' accuracy by its ability to assign high-probability to correct answers. The accuracy can be partially summarized using the AUROC metric as well as a 
receiver operating characteristic curve used where the models
predicted answer being chosen is considered a true positive classification by the model.

%%% Explain more about AUROC metric.

\subsection{Datasets}

In order to understand the the behavior of 
base vs chat models (models fine tuned as conversational agents). We test out models of varying sizes (7b, 13b and in some cases 70b parametes) on a variety of question answering datasets. A brief explanation of the datasets used is given here:

\subsubsection{MMLU}

The Measuring Massive Multitask Language Understanding (MMLU) 
\cite{hendrycks2021measuring} is a massive dataset of multiple choice questions which covers 57 tasks in various subjects including elementary mathematics, US history, computer science, law, and more. For convinience the datasets are grouped into 
four catagories \emph{STEM},  \emph{Humanities}, \emph{Social Sciences}, and \emph{Other}(global facts, marketing, accounting etc.). Questions test the model for both factual and conceptual understanding of the given topics.

\subsubsection{LogicQA}

LogicQA comprehensive dataset, named LogiQA, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. Along with TruthfulQA this dataset shows illustrates limitations of calibration when information is less factual in nature.

\subsubsection{TruthfulQA}

TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. Questions are crafted so that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts.  Of the models 
tested we find this dataset to be the most challenging. 


\section{Model}

The tests are conducted on the 3 to 4 variants of the 
LLama Model. For lower memory footprints we use the LLama 7b  model with roughly 7 billion parameters in two variants one base mode which has not been fine tuned using RLHF as a conversational agent as well as Llama 7b chat a model which has been fine tuned for conversation. Additionally 13b and 70b  variants of the two model types are used where feasible. Generally, we find early trends found in 7b models holding for larger parameter variants.

\section{Results}



\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.60\textwidth}
         \centering 
         \includegraphics[width=1.1\textwidth]{figures/0-shot-MMLU.png}
         \caption{\textbf{0-shot MMLU} }
         \label{fig:0-shot-MMLU}
     \end{subfigure}
     \hfill
         \begin{subfigure}[b]{0.38\textwidth}
         \centering 
         \includegraphics[width=0.9\textwidth]{figures/0-shot-MMLU-roc.png}
         \caption{\textbf{0-shot MMLU ROC}}
         \label{fig:0-shot-MMLU}
    \end{subfigure}  
     \hfill
     \begin{subfigure}[b]{0.60\textwidth}
         \centering
         \includegraphics[width=1.1\textwidth]{figures/5-shot-MMLU.png}
         \caption{\textbf{5-shot MMLU} }
         \label{fig:5-shot-logicqa}
     \end{subfigure}     
     \begin{subfigure}[b]{0.38\textwidth}
         \centering 
         \includegraphics[width=0.9\textwidth]{figures/5-shot-MMLU-roc.png}
         \caption{\textbf{5-shot MMLU ROC} }
         \label{fig:0-shot-MMLU}
    \end{subfigure} 
    
        \caption{Calibration and Accuracy on MMLU Dataset. We note that multi-shot prompting leads 
        to greater accuracy and calibration for fine-tuned chat models.}
        \label{fig:three graphs}
\end{figure*}

\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.60\textwidth}
         \centering \includegraphics[width=1.1\textwidth]{figures/0-shot-MMLU-subjects.png}
         \caption{\textbf{0-shot MMLU by Subject(Chat):} }
         \label{fig:0-shot-MMLU}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.38\textwidth}
         \centering \includegraphics[width=0.9\textwidth]{figures/0-shot-MMLU-subjects-roc.png}
         \caption{\textbf{0-shot MMLU by Subject ROC (Chat):} Performance on MMLU shows poor classification, however multi-shot prompting enhances classification for subjects the model is predisposed to answer}
         \label{fig:0-shot-MMLU-ROC}
    \end{subfigure}
    
     \hfill
     \begin{subfigure}[b]{0.60\textwidth}
         \centering
         \includegraphics[width=1.1\textwidth]{figures/5-shot-MMLU-subjects.png}
         \caption{\textbf{5-shot MMLU by Subject(Chat):}  For subjects which the model can correctly 
         answer multi-shot prompting improves accuracy and calibration}
         \label{fig:5-shot-logicqa}
     \end{subfigure}     
    \hfill 
     \begin{subfigure}[b]{0.38\textwidth}
         \centering \includegraphics[width=0.9\textwidth]{figures/5-shot-MMLU-subjects-roc.png}
         \caption{\textbf{5-shot MMLU by Subject ROC (Chat):} We note large increase in accuracy for 
         Humanities and Other, category with multi-shot prompting}
         \label{fig:0-shot-MMLU}
    \end{subfigure} 
    
        \caption{Calibration Performance of Chat and Base models on the MMLU multiple choice question answering dataset.}
        \label{fig:three graphs}
\end{figure*}

\subsection{Model Calibration By Number of Parameters}

We compare the calibration of the Llama \cite{touvron2023llama} model 
7b-chat and 13b chat models for answering multiple choice questions in 
figure ... 

We note that that the calibration of the model while not perfect improves 
with the size of the model with the 13b-chat model being better calibrated 
and closer to the ideal calibration line than the 7b-chat model. Thus we 
note that models sizes improves not only the performance of the model but
the calibration of the model as well.

\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.60\textwidth}
         \centering \includegraphics[width=1.0\textwidth]{figures/0-shot-logic-qa.png}
         \caption{0-shot performance on LogicQA, we note the base model under-predicts its performance.}
         \label{fig:0-shot-MMLU}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.38\textwidth}
         \centering \includegraphics[width=0.9\textwidth]{figures/0-shot-logic-qa-roc.png}
         \caption{\textbf{0-shot Logic QA ROC:} Bigger fine-tuned modes better accuracy},
         \label{fig:0-shot-MMLU}
    \end{subfigure}  
    
     \hfill
     \begin{subfigure}[b]{0.60\textwidth}
         \centering
         \includegraphics[width=1.0\textwidth]{figures/5-shot-logic-qa.png}
         \caption{\textbf{5-shot Logic QA:} Chat models improve in calibration in 5-shot model, much like MMLU. Base models do not show any such improvement.}
         \label{fig:5-shot-logicqa}
     \end{subfigure}     
    \hfill 
     \begin{subfigure}[b]{0.38\textwidth}
         \centering \includegraphics[width=0.9\textwidth]{figures/5-shot-logic-qa-roc.png}
         \caption{\textbf{5-shot Logic QA ROC:}  note that we do not see significant differences in performance before and after prompting. Fine tuning however improves performance and calibration},
         \label{fig:0-shot-MMLU}
    \end{subfigure} 
     
     
        \caption{Calibration Performance of Chat and Base models on the LogicQA , logical question answering dataset}
        \label{fig:three graphs}
\end{figure*}

\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.60\textwidth}
         \centering \includegraphics[width=1.0\textwidth]{figures/0-shot-truthful_qa.png}
         \caption{\textbf{0-Shot TruthfulQA:} We note that chat models suffer from overconfidence in 0-shot models, moreover model size fails to show significant improvement on TruthfulQA dataset}
         \label{fig:0-shot-truthfulqa}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.38\textwidth}
         \centering \includegraphics[width=0.9\textwidth]{figures/0-shot-truthful_qa-roc-roc.png}
         \caption{\textbf{0-shot TruthfulQA:} Base models marginally outperform fine-tuned chat models 
         in terms of accuracy},
         \label{fig:0-shot-MMLU}
    \end{subfigure} 
     \hfill
     \begin{subfigure}[b]{0.60\textwidth}
         \centering
         \includegraphics[width=1.0\textwidth]{figures/5-shot-TruthQA.png}
         \caption{\textbf{5-shot Truthful QA:} Truthful QA finds chat models overconfident in their prediction after fine-tuning. No large increase in accuracy or calibration is observed.}
         \label{fig:5-shot-truthfulqa}
     \end{subfigure}    
     \hfill
    \begin{subfigure}[b]{0.38\textwidth}
         \centering \includegraphics[width=0.9\textwidth]{figures/5-shot-TruthfulQA-roc.png}
         \caption{\textbf{5-shot Truthful QA:}  Multiple shot prompting worsens performance of fine-tuned models},
         \label{fig:0-shot-MMLU}
    \end{subfigure} 
        \caption{Calibration Performance of Chat and Base models on the Truthful QA , logical question answering dataset}
        \label{fig:three graphs}
\end{figure*}



\subsection{Model Calibration By Fine Tuning}  

We first note that fine tuning improves 0-shot calibration. For both 7b and 13b models as observed in 
Figure: \ref{fig:chat-vs-hf}. Where we note the chat model is closer to the ideal calibration line. Moreover we observe that 
multi-shot prompting as substantially improves the calibration of smaller 7-b model bringing it closer to calibration performance 13-b model used without prompting.


\subsection{Model Calibration By Subject Specialization}  

Another interesting phenomenon is seen for subject specialization on the MMLU bench mark. We note that calibration for 13b models  trends closer to ideal calibration line for more confident response with the exception of STEM fields where the calibration is substantially worse, having us conclude that the model 
is overconfident and incorrect on scientific questions.




\subsection{HumanEval}

\begin{figure}
  \includegraphics[width=0.5\textwidth]{figures/0-shot-7b-human-eval.png}
  \caption{We run a boolean model, to asses ability of model to judge if a proposed solution satisfies a requirement on human eval dataset.}
  \label{fig:human-eval-results}
\end{figure}

\subsection{GSM8k}

GSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.

These problems take between 2 and 8 steps to solve. Solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations  to reach the final answer. A bright middle school student should be able to solve every problem: from the paper, "Problems require no concepts beyond the level of early Algebra, and the vast majority of problems can be solved without explicitly defining a variable." Solutions are provided in natural language, as opposed to pure math expressions. 

\subsection{Trivia QA}



\section{Discussion}

\section{Conclusion}

In this study we attempted to study language model calibration 
under various conditions such as model size, fine tuning and  
task specialization. We found that we were able to replicate the 
observed  calibration behavior of closed models like GPT-3, and 
Claude on the open models like Llama 7b-chat and 13b-chat. 

We note that the calibration of the models  improved with the size 
of the model ...

While calibration is one aspect of querying model understanding it is 
certainly  not the only criteria for evaluating model understanding.  

Other criteria might include models' ability to reason step by step, 
and other demonstrate conceptual understanding by generalizing 
out of distribution. 

\section{Acknowledgements}

\section{References}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendix}
\label{sec:appendix}

This is a section in the appendix.

\end{document}
